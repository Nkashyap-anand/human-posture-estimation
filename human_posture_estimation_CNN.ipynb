{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkashyap-anand/human-posture-estimation/blob/main/human_posture_estimation_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sGsba8RhoccU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TxRKRZ1VsnL2"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6SdTXevNtP9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6c6ace-7c35-4118-8b52-bde78f60785e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['standing', 'bending', 'sitting', 'lying']\n",
            "['standing', 'bending', 'sitting', 'lying']\n"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZIgMjYVVusqt"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KmSzPccHut1c"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset', \n",
        "                transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hqEoXFMwu2BJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de7030c-7718-4479-8fa2-a6de93f81887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 512, 512]) 0\n"
          ]
        }
      ],
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9EHUt56cvbZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe189f0e-ffc2-4c4f-8d66-579dd312d65a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa08c905c10>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "# set the seed of the random number generator to a fixed value\n",
        "# so that when you call for example torch.rand(2), the results will be reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "g1abucmIu8vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eef18a3-aac2-49ff-fbc1-07c452791e3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2880, 960, 960)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# 60:20:20 split\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [2880, 960, 960])\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8rDjk_pLvhuP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size = 128 #start with 64 or 128 and keep doubling it until performance increases "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mNCTgCmFvlPX"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, \n",
        "                      batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "\n",
        "val_dl = DataLoader(val_ds,\n",
        "                    batch_size*2,         #can double the batch size because no gradient decent is done here so more memory is free\n",
        "                    num_workers = 4,\n",
        "                    pin_memory=True)\n",
        "\n",
        "\n",
        "# Num_workers tells the data loader instance how many sub-processes to use for data loading. \n",
        "# If the num_worker is zero (default) the GPU has to weight for CPU to load data.\n",
        "# Theoretically, greater the num_workers, more efficiently the CPU load data and less the GPU has to wait.\n",
        "\n",
        "# pin_memory lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HYoY5pYFv5FN"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'       \n",
        "#matplotlib.rcParams contains some properties in matplotlibrc file. We can use it to control the defaults of almost every property in Matplotlib: figure size and DPI, line width, color and style, axes, axis and grid properties, text and font properties and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AatFo7nfvtn-"
      },
      "outputs": [],
      "source": [
        "#from torchvision.utils import make_grid\n",
        "\n",
        "# Converts a flow to an RGB image. make_grid (tensor[, nrow, padding, â€¦]) Make a grid of images.\n",
        "\n",
        "# def show_batch(dl):\n",
        "#   for images, labels in dl:\n",
        "#     fig, ax = plt.subplots(figsize=(20,10))\n",
        "#     ax.set_xticks([]); ax.set_yticks([])\n",
        "#     ax.imshow(make_grid(images, nrow=16).permute(1,2,0))\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cIsbGG47wAXI"
      },
      "outputs": [],
      "source": [
        "#show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lbCkYUZJwDj4"
      },
      "outputs": [],
      "source": [
        "#show_batch(val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUkRogWNxWL-"
      },
      "source": [
        "## Defining the Model (Convolutional Neural Network)\n",
        "\n",
        "we will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7pDOm-AUxd_a"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "hjuxjvYrxibl"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(3, 8, kernel_size = 3, stride=1, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bOd0k9YrypWd"
      },
      "outputs": [],
      "source": [
        "pool = nn.MaxPool2d(2,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YGb_jWZIysEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b128b07-4622-4fe6-cae5-3117dcef15af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image.shape:  torch.Size([128, 3, 512, 512])\n",
            "out.shape torch.Size([128, 8, 512, 512])\n",
            "out.shape after pooling torch.Size([128, 8, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_dl:\n",
        "  print('image.shape: ', images.shape)\n",
        "  out = conv(images)\n",
        "  print('out.shape', out.shape)\n",
        "  out = pool(out)\n",
        "  print('out.shape after pooling', out.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bVJiusPyyw3v"
      },
      "outputs": [],
      "source": [
        "simple_model = nn.Sequential(\n",
        "    nn.Conv2d(3,8, kernel_size=3, stride=1, padding=1),\n",
        "    nn.MaxPool2d(2,2)\n",
        ")\n",
        "\n",
        "# The objective of nn. Sequential is to quickly implement sequential modules such that \n",
        "# you are not required to write the forward definition, \n",
        "# it being implicitly known because the layers are sequentially called on the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cjcjJYtgy2ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ee300e-dea1-403a-de0c-f88cb2cbf3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image.shape:  torch.Size([128, 3, 512, 512])\n",
            "out.shape torch.Size([128, 8, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_dl:\n",
        "  print('image.shape: ', images.shape)\n",
        "  out = simple_model(images)\n",
        "  print('out.shape', out.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj67fi1Wy6WW"
      },
      "source": [
        "\n",
        "## Let's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qGy_5w_My-xl"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        \n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."
      ],
      "metadata": {
        "id": "er7IKVjBz63Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class human_posture_estimation_CNN(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # output: 3 x 512 x 512\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
        "            # output: 32 x 512 x 512\n",
        "            nn.ReLU(),\n",
        "            # output: 32 x 512 x 512\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            # output: 64 x 512 x 512\n",
        "            nn.ReLU(),\n",
        "            # output: 64 x 512 x 512\n",
        "            nn.MaxPool2d(2, 2), \n",
        "            # output: 64 x 256 x 256\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 128 x 128\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 64 x 64\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*64*64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 4))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "uwAPnjSgz7Or"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = human_posture_estimation_CNN()\n",
        "model"
      ],
      "metadata": {
        "id": "8G5oMsrI1fGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad47869-7ff0-403c-92d5-7adbe927dd7e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "human_posture_estimation_CNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=1048576, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. "
      ],
      "metadata": {
        "id": "6_Rv35Tt1td4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dl:\n",
        "  print('image.shape', images.shape)\n",
        "  out = model(images)\n",
        "  print('out.shape', out.shape)\n",
        "  print('out[0]: ', out[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "Wd-gB2kp1wJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b01f93c-e3db-4182-f1d4-59fa81fccbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image.shape torch.Size([128, 3, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "qpx8HA_k1zMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "ZG2AoomT12OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "to_device(model, device)"
      ],
      "metadata": {
        "id": "N9lotqdI13vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set."
      ],
      "metadata": {
        "id": "Kdmn7Lv815mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "  model.eval()\n",
        "  outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
        "  history = []\n",
        "  optimizer = opt_func(model.parameters(), lr)\n",
        "  for epoch in range(epochs):\n",
        "    # traiing phase\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for batch in train_loader:\n",
        "      loss = model.training_step(batch)\n",
        "      train_losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    # validation phases\n",
        "    result = evaluate(model, val_loader)\n",
        "    result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "    model.epoch_end(epoch, result)\n",
        "    history.append(result)\n",
        "  return history"
      ],
      "metadata": {
        "id": "StGouijs18CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = to_device(human_posture_estimation_CNN(), device)"
      ],
      "metadata": {
        "id": "G4wQvpV-2Cfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, val_dl)"
      ],
      "metadata": {
        "id": "_B-B9Vgx2JdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "1oe3ylZO2Pwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
      ],
      "metadata": {
        "id": "PlBxcini2Tw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');"
      ],
      "metadata": {
        "id": "TMZ4L-kc76zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ],
      "metadata": {
        "id": "2h-my5QB79eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img, model):\n",
        "    # Convert to a batch of 1\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    # Get predictions from model\n",
        "    yb = model(xb)\n",
        "    # Pick index with highest probability\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    # Retrieve the class label\n",
        "    return dataset.classes[preds[0].item()]"
      ],
      "metadata": {
        "id": "aydIKQ5t7_07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_ds[0]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "print('Label: ', dataset.classes[label], ', Predicted', predict_image(img, model))"
      ],
      "metadata": {
        "id": "_DJOo01I8Hqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DeviceDataLoader(DataLoader(test_ds, batch_size*2), device)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "metadata": {
        "id": "qXRodzAd92GG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "human_posture_estimation_CNN",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ulmosd5TQI3A67Wed-kiyJZS3t0mk3R4",
      "authorship_tag": "ABX9TyPHmUDQVBG9eUk7qQOTyVLQ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}